---
title: "PW7"
author: "LAMAUVE-JEANNEAU-FLOCEA-GRAU"
date: "25/11/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## k-means clustering

```{r}
# 1. Dowloading the dataset:
ligue1 <- read.csv("http://mghassany.com/MLcourse/datasets/ligue1_17_18.csv", row.names=1, sep=";")

# 2. Printing the first two rows of the dataset
knitr::kable(ligue1[1:2,])
# Printing the number of features (number of columns)
ncol(ligue1)

# pointsCards:
# 3. New dataset with Points and yellow cards :
pointsCards=ligue1[c('Points', 'yellow.cards')]
pointsCards

# 4. Applying k-means on pointsCards with k = 2 and number of iterations = 20 :
set.seed(123)
km=kmeans(pointsCards,centers=2,iter.max = 20)

# 5. Printing km :
km
# We have 2 clusters, the first one contains 4 teams and the second contains 16.
# We can know the sum of squares by cluster and other components such as the 
# coordinates the centers.

# 6. Printing the coordinates of the clusters' centroids :
km$centers

# 7. Plotting the data :
plot(pointsCards$Points,pointsCards$yellow.cards,col=km$cluster,pch=19,cex=2)

# 8. Adding centroids and naming the observations :
points(km$centers,col=1:2,pch=3,cex=3,lwd=3)
text(pointsCards$Points,pointsCards$yellow.cards,rownames(pointsCards), pos = 1)

# 9. Re-run k-means on pointsCards with 3 clusters
km3 = kmeans(pointsCards,centers=3,iter.max = 20)
# Plotting the data :
plot(pointsCards$Points,pointsCards$yellow.cards,col=km3$cluster,pch=19,cex=2)
# Adding centroids and naming the observations :
points(km3$centers,col=1:3,pch=3,cex=3,lwd=3)
text(pointsCards$Points,pointsCards$yellow.cards,rownames(pointsCards), pos = 1)

# Re-run k-means on pointsCards with 4 clusters
km4 = kmeans(pointsCards,centers=4,iter.max = 20)
# Plotting the data :
plot(pointsCards$Points,pointsCards$yellow.cards,col=km4$cluster,pch=19,cex=2)
# Adding centroids and naming the observations :
points(km4$centers,col=1:4,pch=3,cex=3,lwd=3)
text(pointsCards$Points,pointsCards$yellow.cards,rownames(pointsCards), pos = 1)

# 10. Visualizing the "within groups sum of squares" of the k-means clustering results
wss <- (nrow(pointsCards)-1)*sum(apply(pointsCards,2,var))
for (i in 1:15) wss[i] <- sum(kmeans(pointsCards,centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
# 11. Modifying the code to visualize the ‘between_SS / total_SS’.
for (i in 1:15) wss[i] <-(kmeans(pointsCards,centers=i)$betweenss)/(kmeans(pointsCards,centers=i)$totss)*100
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
# From k = 3, we obtain more than 80% of the total variation so we can choose k = 3.

# Ligue 1
# 12. Scaling the dataset :
ligue1_scaled = scale(ligue1,center=TRUE,scale=TRUE)

# 13. Applying k-means on ligue1 :
km.ligue1 = kmeans(ligue1,centers=3,iter.max = 20)
km.ligue1
km.ligue1.scaled = kmeans(ligue1_scaled,centers=3,iter.max=20)
km.ligue1.scaled

# 14. Printing the number of obs in each cluster :
table(km.ligue1$cluster)
table(km.ligue1.scaled$cluster)
# We obtain the same results as the kmeans() function

# 15. Applying PCA on ligue1 :
pcaligue1 = prcomp(ligue1,center=TRUE,scale=TRUE)
# No, because we will obtain the same results if we give center and scale = TRUE

# 16. Plotting the observations with biplots :
library('factoextra')
fviz_pca_biplot(pcaligue1)
# We can see that the two PCs contribute for 73.6% Of the total variation.

# 17. Visualizing the teams on the first two PC and color them :
fviz_cluster(km.ligue1, data = ligue1, 
              palette = c("red", "blue", "green"),
              ggtheme = theme_minimal(),
              main = "Clustering Plot"
)

# 18. Applying k-means on the first two PCs
ind.coord = as.data.frame(get_pca_ind(pcaligue1)$coord)
ligue1.PC1.PC2 = ind.coord[, c('Dim.1','Dim.2')]
km.ligue1.pca = kmeans(ligue1.PC1.PC2, centers=3,iter.max=20)

fviz_cluster(km.ligue1.pca, data = ligue1, 
              palette = c("red", "blue", "green"),
              ggtheme = theme_minimal(),
              main = "Clustering Plot"
)

# Implementing k-means

# 19. Plotting the observations :
x1 = c(1,1,0,5,6,4)
x2 = c(4,3,4,1,2,0)
plot(x1,x2)

# 20. Randomly assigning cluster labels :
cluster = sample(1:2, 6, replace = TRUE)
cluster
plot(x1,x2,col=cluster)
# 21. Computing the centroid for each cluster :
x1.centroid1=-1
x2.centroid1=-1
x1.centroid2=-1
x2.centroid2=-1
repeat{
  x1.cluster1 = c()
  x2.cluster1 = c()
  x1.cluster2 = c()
  x2.cluster2 = c()
  for (i in 1:6)
  {
    if(cluster[i]==1)
      {
      x1.cluster1 = c(x1.cluster1,x1[i])
      x2.cluster1 = c(x2.cluster1,x2[i])
      }
    if(cluster[i]==2)
      {
      x1.cluster2 = c(x1.cluster2,x1[i])
      x2.cluster2 = c(x2.cluster2,x2[i])
      }
  }
  old.centroid1=c(x1.centroid1,x2.centroid1)
  old.centroid2=c(x1.centroid2,x2.centroid2)
  x1.centroid1 = mean(x1.cluster1)
  x2.centroid1 = mean(x2.cluster1)
  x1.centroid2 = mean(x1.cluster2)
  x2.centroid2 = mean(x2.cluster2)
  plot(x1,x2,col=cluster)
  points(x1.centroid1, x2.centroid1,col=1,lwd=3)
  points(x1.centroid2, x2.centroid2,col=2,lwd=3)
  
  # 22. Computing the euclidean distance :
  euclidean_distance = function(x1, y1, x2, y2){
    sqrt((x1 - x2)^2 + (y1 - y2)^2)
  }
  
  # 23. Assigning each obs to the nearest cluster
  for (i in 1:6)
  {
    d1 = euclidean_distance(x1[i],x2[i],x1.centroid1,x2.centroid1)
    d2 = euclidean_distance(x1[i],x2[i],x1.centroid2,x2.centroid2)
    if(d1<d2){cluster[i]=1}
    else{cluster[i]=2}
  }
  plot(x1,x2,col=cluster)
  points(x1.centroid1, x2.centroid1,col=1,lwd=3)
  points(x1.centroid2, x2.centroid2,col=2,lwd=3)
  new.centroid1=c(x1.centroid1,x2.centroid1)
  new.centroid2=c(x1.centroid2,x2.centroid2)
  
  # 24. Repeat 21-23 until any centroid moves
  if((old.centroid2==new.centroid2) & (old.centroid1==new.centroid1)){
    break
  }
}

```
## Hierarchical clustering
```{r}
# Hierarchical clustering

# 1. Importing the dataset :
iris_data = read.csv("C:/Users/Quentin/Desktop/ML_7/iris.data")

# 2. Choosing randomly 40 observations
set.seed(123)
index = sample(1:nrow(iris), 40)
sampleiris = iris_data[index,]

# 3. Euclidean distance between the flowers
D = dist(sampleiris[,-5])

# 4. Contructing a dendrogram
dendro.avg = hclust(D, method = "average")

# 5. Plotting it
plot(dendro.avg)

# 6. Plotting it again
plot(dendro.avg, hang=-1, label=sampleiris$class)

# 7. Cutting the dendrogram
groups.avg = cutree(dendro.avg,k=3)

# 8. Visualize the cut tree
plot(dendro.avg, hang=-1, label=sampleiris$class)
rect.hclust(dendro.avg,3)

# 9.
table(groups.avg,sampleiris$class)

# 10. 
D2 = dist(iris_data[,-5])

# a)

dendro150.avg = hclust(D2, method = "average")
groups150.avg = cutree(dendro150.avg,k=3)

table(groups150.avg,iris_data$class)

# b)
dendro150.comp = hclust(D2, method = "complete")
groups150.comp = cutree(dendro150.comp,k=3)

table(groups150.comp,iris_data$class)

# c)
dendro150.sing = hclust(D2, method = "single")
groups150.sing = cutree(dendro150.sing,k=3)

table(groups150.sing,iris_data$class)

# We can conclude that in our case :
# average > single > complete
```


